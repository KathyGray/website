extends ../../lab
include ../../mixins/url.pug
include ../../mixins/title.pug
include ../../mixins/userinfo.pug
include ../../mixins/warning.pug
include ../../mixins/shortanswer.pug
include ../../mixins/multiplechoice.pug

block content
    +title("MATH 450 Chapter 4-Part 1")
    +warning("Leaving or refreshing this page might lose your work.")

    form(action="/math450/labs/submit" method="post" class="needs-validation" novalidate)
        input(type="hidden" name="_csrf" value= csrfToken)
        input(type="hidden" name="lab" value= labId)
        // Due date is in UTC
        input(type="hidden" name="duedate" value="2020-06-25T07:00:00")
        +userinfo()

        h4 Likelihoods

        p. 

            This section introduces us to the idea of likelihood.

        p.

            #[strong Section 4.2]:

        p.

            The first and most important choice in a Bayesian analysis is which probability model to use to describe a given process. Probability models are characterise by a set of parameters which, when varied, generate a range of different system behaviors. In Bayesian inference we wish to determine a posterior belief in each set of parameter values. This means that in Bayesian inference we instead hold the data constant, and vary the parameter values. When we do this, the distribution no longer sums or integrates to 1. To acknowledge this distinction in Bayesian inference we avoid using the term #[i probability distribution] in favour of #[i likelihood]. This chapter is devoted to a discussion of likelihood.

        p.

            #[strong 4.3: What is a likelihood?]

        p.

            In all statistical inference, we use an idealised model to approximate a real-world process. This model is then used to test hypotheses about the world. In Bayesian statistics, the evidence for a particular hypothesis is summarised in a posterior probability distribution. Bayes' rule explains how to compute the posterior probability distribution: $$p(\theta|data)=\frac{p(data|\theta)\times p(\theta)}{p(data)}$$

        p.

            To understand this rule we first need to know what is meant by the numerator term, \(p(data|\theta)\), which Bayesians call a #[i likelihood].

        p.

            #[strong Maximum Likelihood Estimation]

        p.

           In estimation we take a random sample from the distribution to elicit some information about the unknown parameter \(\theta\).  That is, we repeat the experiment \(n\) independent times, observe the sample \(X_{1},X_{2},...,X_{n}\) and try to estimate the parameter \(\theta\).  We would like the estimate of \(\theta\) to be as closed to \(\theta\) as possible.

        p.

           This section will discuss the method of #[i maximum likelihood].  Let \(X_{1},...,X_{n}\) be a random sample from \(f_{X}(x)\) and let \(L(\theta)\) be defined as follows: $$L(\theta)=f_{x_{1},x_{2},...,x_{n}}(x_{1},x_{2},...,x_{n})=f_{X_{1}}(x_{1})f_{X_{2}}(x_{2})\cdots f_{X_{n}}(x_{n})=\prod^{n}_{i=1}f_{X_{i}}(x_{i})$$

        p.

            This is called the likelihood function.

        p.

            #[strong Example]:  A simple numeric example will illustrate how we intend to make use of the likelihood function.  Suppose three observations yield the values \(x_{1}=2\), \(x_{2}=4\), and \(x_{3}=3.6\) are drawn from the pdf $$f_{X}(x;\theta)=\frac{1}{\theta}e^{-x/\theta}$$

        p.

            #[+url("https://media.csuchico.edu/media/Math+450+MLE+Simple+Example/0_11redujg")]

        p.

            Thus, the likelihood function is $$\frac{1}{\theta}e^{-9.6/\theta}$$

        p.

            The graph of the likelihood function is given below:

        p.

        a: img(src="/max_likelihood.PNG" alt="Max Likelihood graph")    

        p.

        +shortanswer("answer01", "What value is the value for theta according to the graph?", "1")

        p.

            That is, for the given three observations, the joint pdf is maximized when the parameter is assigned the value 3.2.  This amounts to saying that, in terms of the joint pdf, the value of \(\theta=3.2\) ``best explains'' or is ``most compatible with'' the data.  It would be reasonable then to set the value for the estimate of \(\theta\) equal to 3.2.

        p.

            #[strong Maximum-likelihood Estimation]

        p.

            Let \(X_{1},X_{2},...,X_{n}\) be a random sample from \(f(x|\theta)\) and let \(L(\theta)\) be the corresponding likelihood function.  As a general procedure for constructing estimators, we will look for the #[i w] that maximizes \(L(\theta)\).  Any value that does is said to be the #[i maximum-likelihood estimate] (or MLE) for \(\theta\).

        p.


            In many cases of interest, \(L(\theta)\) is differentiable and its maximization is a familiar calculus problem:  the basic procedure is to differentiate \(L(\theta)\) with respect to $\theta$, set the derivative equal to 0, and solve for \(\theta\).  The usual second derivative test can be used to verify that \(\theta\) is, in fact, #[i maximizing] \(L(\theta)\).  Often it is easier to maximize ln \(L(\theta)\) rather than \(L(\theta)\).

        p.

            #[strong Example]

        p.

            Let \(X_{1},X_{2},...,X_{n}\) be a random sample from the Poisson distribution, $$f(x|\lambda)=\frac{e^{-\lambda}\lambda^{x}}{x!}$$

        p.

            for #[i x]=0,1,2,....

        p.
        
            #[+url("https://media.csuchico.edu/media/Math+450+MLE+Poisson/0_qax11lsw")]

        p.

            #[strong You try it]

        p.

            If a series of independent Bernoulli trials is terminated with the occurrence of the first success, the probability function #[i K], the length of the series will be given by the #[i geometric distribution], $$f(k|p)=\textmd{Pr}(K=k)=\left(1-p\right)^{k-1}p$$

        p.

            with #[i k=1,2,...] and #[i p] is the (unknown) probability of success at any trial.  If #[i n] such series are observed, the data will consist of #[i n] lengths, \(k_{1},k_{2},...,k_{n}\).

        p.

        +shortanswer("answer02", "What is the maximum likelihood estimator for p?", "1")

        p.

            #[strong You try it]

        p.

            Suppose that \(X_{1},...,X_{n}\) form a random sample from an exponential distribution for which the value of the parameter \(\beta\) is unknown.  Find the M.L.E. of \(\beta\).

        +shortanswer("answer03", "What is the maximum likelihood estimator?", "1")

        button(class="btn btn-md btn-primary" type="submit") submit
