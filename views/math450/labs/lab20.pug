extends ../../lab
include ../../mixins/url.pug
include ../../mixins/title.pug
include ../../mixins/userinfo.pug
include ../../mixins/warning.pug
include ../../mixins/highlighting.pug
include ../../mixins/code.pug
include ../../mixins/katex.pug
include ../../mixins/numericanswer.pug

block head
    +katex()
    +highlighting()

block content
    +title("MATH 450 Chapter 13-Part 2")
    +warning("Leaving or refreshing this page might lose your work.")

    form(action="/math450/labs/submit" method="post" class="needs-validation" novalidate)
        input(type="hidden" name="_csrf" value= csrfToken)
        input(type="hidden" name="lab" value= labId)
        // Due date is in UTC
        input(type="hidden" name="duedate" value="2020-06-25T07:00:00")
        +userinfo()

        h4 Chapter 13: Random Walk Metropolis Continued

        p.

            #[+url("https://youtu.be/U561HGMWjcw")]

        p.

            #[strong Defining the Random Walk Metropolis Algorithm]: We will now define the random walk metropolis algorithm. Imagine that we have a posterior whose density is know to be a proportion: $$p(\theta|data)\propto p(data|\theta)\times p(\theta)$$

        p.

            We would like to build an understanding of the shape and properties of the posterior. For now, imagine that $\theta$ is a one-dimensional continuous parameter. 

        p.

            We imagine that we do a random walk through the posterior space, and at each point in time decide where next to step based on our current position and the shape of the posterior. If we design the stepping routine correctly, and take enough steps, we hope to build up an approximate global picture of the posterior from the local density of samples across parameter space.

        p.

            If we always move to the next step without considering the height, we call this the #[i drukard's random walk], the result of which is the uniform distribution. If we always move to the highest step then we would end up with a distribution that is heavily centered at one number. 

        p.

            Therefore, we need an algorithm that pays just the right amount of attention to the posterior shape. It turns out that if w were to follow the decision-making rule, we then get the perfect balance: $$r=\left\{\begin{array}{ll}
            1,&\textmd{if} p\left(\theta_{t+1}|data\right)\geq p\left(\theta_{t}|data\right)\\
            \frac{p\left(\theta_{t+1}|data\right)}{p\left(\theta_{t}|data\right)}, & \textmd{if} p\left(\theta_{t+1}|data\right)< p\left(\theta_{t}|data\right)\end{array}\right.$$

            where #[i r] is the probability that we accept the proposed point \(\theta_{i+1}\) as our next sample value. This algorithm ensures that we sample from a parameter location in relation to its relative height. This rule only requires the relative height of the posterior at each step. This means that we can avoid the complication of calculating the denominator and instead approximate all quantities of interest form our sample.

        p.

            #[strong Intuition behind the accept-reject rule of Metropolis]: We will start our MCMC at some random point in the parameter space, which will not be indicative of the posterior distribution. Often we start our chains in areas of higher density so that the chain does not meander enough around the flat stretches. We hope that by doing repeated steps that our distribution of sample values converges to the posterior distribution. 

        p.

            The rule dictates that we always move to a new parameter value if its posterior density is higher there, and that we only move probabilistically if its value is lower, with a probability given by the ratio of the proposed density at the new location compared to its current value. No other rule will work.

        p.

            #[strong Efficiency of Convergence: The Importance of Choosing the Right Proposal Scale]:

        p.

            For balance to be satisfied for our Metropolis algorithm, we require that the proposal distribution be symmetric. But what sorts of distribution might we actually use here? A common choice is a normal distribution. Since this distribution is symmetric about its mean, the probability of choosing \(\theta_{a}\) from \(\theta_{b}\) is the same as the other way around. The parameter that we look at for determining step size is \(\sigma\) from the normal distribution that is centered on the previous value. For instance, suppose that we are on \(\theta_{t-1}\). Our step size for determining the next step would be drawn from \(N(\theta_{t-1},\sigma)\). After determining the step size we would calculate r: $$r=\frac{P(X|\theta_{t})\times P(\theta_{t})}{P(X|\theta_{t-1})\times P(\theta_{t-1})}$$

        p.

            Depending on the ratio we accept the new location of the parameter or probabilistically reject the new location. The performance of the algorithm depends on choosing an appropriate value for $\sigma$. If $\sigma$ is too small we accept a large number of steps but it takes a long time to observe the posterior space. If $\sigma$ is too big then we reject a large number of steps and we move around inefficiently. By picking an appropriate value for $\sigma$ we find a balance between the two.

        p.

            #[strong Tuning the proposal distribution]: 

        p.

            #[+url("https://youtu.be/3E33AScbU4s")]

            What is the optimal rate of acceptance for our MCMC sampler to converge on the posterior at the fastest rate? For the Metropolis algorithm an acceptance rate of 0.44 for one-dimensional models and 0.23 for models with more than one dimensions leads to optimal convergence as measured by a number of criteria, across a range of densities.

        p.

            In reality, it is ofen easiest to split the algorithm into two stages:
        
        p.

            1. Run a training algorithm to find the optimal proposal distribution parameters.

        p.

            2. Initialise new chains and run them using a proposal distribution with tuned parameters.

        p.

            The training step is where we start a Metropolis Markov chain and update its proposal step size at regular intervals in order to bring the acceptance rate closer to the ideal of 0.44. We start two chains-one with a step size that is too low, the other taking steps that are too long-and then subtract a small amount (\(\epsilon_{t}\) from the step size if the acceptance rate is too low or increase the step size if it is too high. 

        p.

            #[strong Metropolis-Hastings]: Suppose we believe that the returns of a single stock \(Y_{t}\) can be modelled by a normal sampling distribution with mean \(\mu\) and standard deviation \(\sigma\) where \(\mu\) is the mean stock return and \(\sigma\) is its standard deviation. While \(\mu\) in theory could be negative, it is not meaningful to have \(\sigma\). Unfortunately, we can no longer use the Metropolis algorithm, since a symmetric proposal distribution can propose a negative value for \(\sigma\). 

        p.

            The Metropolis algorithm does not work when we have boundaries in parameter values.

        p.

            The Metropolis-Hastings algorithm is a modification that we make to the basic Metropolis algorithm to allow for an asymmetric proposal distribution.

        p.

            For a Metropolis sampler, we used a normal distribution whose mean equalled the current parameter value. What we need is a proposal distribution that always produces parameter values in the allowed region of parameter space. This means that we need an asymmetric proposal distribution.

        p.

            #[strong Metropolis-Hastings algorithm]: The Metropolis algorithm does not work when we have boundaries in parameter values. The Metropolis-Hastings algorithm is a modification that we make to the basic Metropolis algorithm to allow for an asymmetric proposal distribution.

        button(class="btn btn-md btn-primary" type="submit") submit
