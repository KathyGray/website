extends ../../lab
include ../../mixins/url.pug
include ../../mixins/title.pug
include ../../mixins/userinfo.pug
include ../../mixins/warning.pug
include ../../mixins/shortanswer.pug
include ../../mixins/multiplechoice.pug
include ../../mixins/katex.pug

block head
    +katex()

block content
    +title("MATH 450 Chapter 2-Part 2 Notes")
    +warning("Leaving or refreshing this page might lose your work.")

    form(action="/math450/labs/submit" method="post" class="needs-validation" novalidate)
        input(type="hidden" name="_csrf" value= csrfToken)
        input(type="hidden" name="lab" value= labId)
        // Due date is in UTC
        input(type="hidden" name="duedate" value="2020-06-25T07:00:00")
        +userinfo()

        h4 The Purpose of Statistical Inference

        p.

            How much does a particular drug affect a patient's condition? What can an average student earn after obtaining a college eduction? Will the Democrats win the next presidential election?

        p.

            In life, we develop theories and use these to make predictions, but testing those theories is not easy. Statistical inference is the logical framework which we can use to trial our beliefs about the noisy world against #[i data]. We formalize our beliefs in models of #[probability]. The models are probabilisitic because we don't know all the parts of system, meaning we can't say for certain whether something will or will not happen. Suppose that we are evaluating the efficacy of a drug in a trial. Before we carry out the trial, we  might believe that the drug will cure 10% of people. We cannot say which 10% of people will be cured because we do not know enough about the disease or patient to say exactly whom. Statistical inference allows us to test this belief against the data we obtain in a clinical trial.

        p.

            There are two predominant schools of thought for carrying out this process of inference: Frequentist and Bayesian.

        h4 The World According to Frequentists

        p.

            In Frequentist statistics, we suppose that our sample of data is the result of one of an infinite number of exactly repeated experiments. Any conclusion we draw from this approach are based on the supposition that events occur with probabilities, which represent long-run frequencies with which those events occur in an infinite series of experimental repetitions. For example, if we flip a coin, we take the proportion of heads observed in an infinite number of throws as the defining probability of obtaining heads. Frequentist suppose that this probability actually exists, and is fixed for each set of coin throws we carry out. In Frequentist statistics the data are assumed to be #[ i random] and results from #[i sampling] from a fixed and defined #[population] distribution. \\


        h4 The World According to Bayesians

        p.

            Bayesians do not imagine repetitions of an experiment in order to define and specify a probability. A probability is merely taken as a a measure of certainty in a particular belief. For Bayesians the probability of throwing a #[ i heads] measures and quantifies our underlying belief that before we flip the coin it will land this way.

        p.

            For Bayesians, probabilities are seen as an expression of subjective beliefs, meaning that they can be updated in light of new data. The formula invented by the Reverand Thomas Bayes provides the only logical manner in which to carry out this updating process. Bayes' rule is central to Bayesian inference whereby we use probabilities to express our uncertainty in parameter values after we observe data.

        p.

            Bayesians assume that, since we are witness to the data, it is fixed and therefore does not vary. We do not need to imagine that there are infinite number of possible samples, or that our data are the undetermined outcome of some random process of sampling. Because we never know the exact value of a parameter, the parameter is viewed as a quantity that is probabilistic.

        h4 Do parameters actually exist and have a point value?

        p.

            For Bayesians, the parameters of the system are taken to vary, whereas the known part of the system (the data) is taken as a given. Frequentist statisticians, on the other hand, view the unseen part of the system (the parameters) as being fixed and the known parts of the system (the data) as varying. Which of these views you  prefer comes down to how you interpret the parameters in a statistical model.

        p.


            In the Bayesian approach, parameters can be viewed from two perspectives: Either we view the parameters as truly varying, or we view our knowledge about the parameters as imperfect.

        p.

            The Frequentist perspective is less flexible and assumes that these parameters are constant, or represent the average of a long run of identical experiments. There are occasions when we might think that this is a reasonable assumption. For example, if our parameter represented the probability that an individual taken at random has dyslexia, it is reasonable to think of this value as fixed. However, the Bayesian view can also handle this situation. These paramters can be assumed fixed but we are uncertain of their value before we measure them and use a probability distribution to reflect that uncertainty.

        h4 Frequentist and Bayesian Inference

        p.

            The Bayesian inference process is the only logical and consistent way to modify our beliefs to account for new data. Before we collect the data we have a probabilistic description of our beliefs, which we call a #[i prior]. We then collect data, and together with a model describing our theory, Bayes' formula allows us to calculate our post-data or #[ iposterior] belief.

        p.

            Bayesian analysis is a set of statistical techniques based on inverse probabilities calculated from Bayes' Theorem.  In particular, Bayesian statistics provide formal methods for incorporating prior knowledge into the estimation of unknown parameters.

        p.

            A major difference between Bayesian analysis and non-Bayesian analysis (everything we have learned thus far) is the assumptions associated with unknown parameters.  In a non-Bayesian world, unknown parameters are viewed as constants; in a Bayesian world, unknown parameters are viewed as random variables which means they have pdfs.

        p.

            At the onset in a Bayesian analysis, the pdf assigned to the parameter may be based on little or no information or may be based on one's beliefs of possible values and is referred to as the #[prior distribution].  As soon as some data are collected, it becomes possible via Bayes' Theorem to revise and refine the pdf ascribed to the parameter.  Any such updated pdf is referred to as a #[i posterior distribution].

        h4 Bayesian Inference Via Bayes' Rule

        p.

            Bayes' rule tells us how to update our prior beliefs in order to derive better, more informed, beliefs about a situation in light of new data. In Bayesian inference, we test hypotheses about the real world using these posterior beliefs. As part of this process, we estimate characteristics that interest us,which we call #[i parameters] that are then used to test such hypotheses. From this point onward we will use \(\theta\) to represent the unknown parameter(s) which we want to estimate.

        p.

            The Bayesian inference process uses Bayes' rule to estimate a probability distribution for those unknown parameters after we observe the data.  It is sufficient for now to think of probability distributions as a way to represent uncertainty for unknown quantities.

        p.

            Bayes' rule as used in statistical inference is of the form: $$p(\theta|data)=\frac{p(data|\theta)\times p(\theta)}{p(data)}$$

        p.

            where we use #[i p] to indicate a probability distribution which may represent either probabilities or, more usually, probability densities.

        h4 Likelihoods

        p.

            The term $$p(data|\theta)$$ is called the likelihood which is common in both frequentist and Bayesian statistics. The likelihood tells us the probability of getting the data for a given value of a parameter. If we are searching for a reasonable value for a parameter and we collect data we might pick the value for \(\theta\) that has the largest likelihood given our data. This is called the maximum likelihood estimate or MLE.

        h4 Priors

        p.

            The term \(p(\theta)\) is called the prior distribution. This represents our knowledge about a parameter #[strong before we collect data]. This differs from frequentist statistics because in frequentist statistics we do not incorporate any prior knowledge about a parameter into the data analysis.

        h4 The Denominator

        p.

           The final term on the right-hand side of the above expression is the denominator, $$p(data)$$. This represents the probability of obtaining our particular sample of data if we assume a particular model and prior. The denominator is fully determined by our choice of prior and likelihood function.

        h4 Posterior

        p.

            The posterior probability distribution $$p(\theta|data)$$ is the main goal of Bayesian inference. The posterior distribution summarises our uncertainty over the value of a parameter. If this distribution is narrow, then this indicates that we have greater confidence in our estimates of the parameter's value. More narrow posterior distributions can be obtained by collecting more data. The more data that is collected, the less impact the prior exerts on posterior distributions. The posterior distribution  is also used to predict future outcomes of an experiment and for model testing.

        h4 Example

        p.

            #[+url("https://media.csuchico.edu/media/Math+450-Simple+Bayes%27+Stat/0_q1r6dck0")]

        p.

            Suppose a retailer is interested in modeling the number of calls arriving at a phone bank in a five-minute interval.  We can establish that the Poisson distribution would be the pdf of choice.  But what value should be assigned to the Poisson parameter, \(\lambda\)?

        +shortanswer("answer01", "How would a frequentist estimate \(\lambda\)?", "2")

        p.

            Suppose we know that an examination of logs for the past several months suggests that \(\lambda\) equals 10 about three-quarters of the time, and it equals 8 about one-quarter of the time.  Described in Bayesian terminology, the rate parameter is a random variable, \(\Lambda\), and the (discrete) prior distribution for \(\Lambda\) is defined by the two probabilities 0.25 and 0.75 .

        p.

            Now suppose certain facets of the retailer's operation have recently changed.  Those changes may very well affect the distribution associated with the call rate.  Updating the prior distribution for \(\Lambda\) requires (a) some data and (b) an application of Bayes' Theorem.  The retailer decides to construct a posterior distribution for \(\Lambda\) on the basis of a single observation.  To that end, a five-minute interval is preselected at random and the corresponding value for [i X] is found to be 7.  How should we revise the previous probabilities for \(\Lambda\)?

        p.

            Notice that the posterior distribution for \(\Lambda\) has changed in a way that makes sense intuitively.  Since x=7 is more consistent with \(\Lambda\)=8 than with \(\Lambda\)=10, the posterior pdf has increased the probability that \(\Lambda\)=8.\\


        button(class="btn btn-md btn-primary" type="submit") submit
