extends ../../lab
include ../../mixins/url.pug
include ../../mixins/title.pug
include ../../mixins/userinfo.pug
include ../../mixins/warning.pug
include ../../mixins/shortanswer.pug
include ../../mixins/multiplechoice.pug
include ../../mixins/numericanswer.pug
include ../../mixins/katex.pug

block head
    +katex()

block content
    +title("MATH 450 Chapter 10")
    +warning("Leaving or refreshing this page might lose your work.")

    form(action="/math450/labs/submit" method="post" class="needs-validation" novalidate)
        input(type="hidden" name="_csrf" value= csrfToken)
        input(type="hidden" name="lab" value= labId)
        // Due date is in UTC
        input(type="hidden" name="duedate" value="2020-06-25T07:00:00")
        +userinfo()

        h4 Chapter 10: Evaluation of Model Fit:

        p.

            Models are interpretations of real life, and are reduced versions of it. However, a good model can suggest valuable strategies for navigating the real world. This begs the questions, #[What is a good model?]. Such a model should incorporate just enough realism so that we trust its outputs while being simple enough to allow interpretation and extrapolation. But how can we determine whether or not it meets that criteria?

        p.

            The Bayesian testing framework which allows us to judge the fit of a model to data is known as #[i posterior predictive checks](PPCs). The idea is that if the model is a good fit to the data, then fake data that we generate from the fitted model should look indistinguishable from the real thing. PPCs will allow us to understand the ways in which a model is good and the ways in which it is deficient.

        p.

            #[strong From Posterior to Predictions by Samping]

        p.

            Because of the two scores of uncertainty included in our model-the parameter uncertainty and sampling variability -the uncertainty of the Bayesian predictive distribution is typically greater than the Frequentist equivalent approach to forecasting. By ignoring any uncertainty in the parameter's value, the Frequentist approach produces predictive intervals that are overly confident.

        p.

            The two uses for posterior predictive distributions are

        p.

            1. Predict future values

        p.

            2. Check appropriateness of model

        p.

            This section outlines what is involved in this process.

        p.

            There are two sources of uncertainty in prediction.

        p.

            1. Uncertainty in the distribution of the parameter.

        p.

            2. Uncertainity in the model for our data.

        p.

            The first of these uncertainties is represented by the posterior distribution. The second is represented by our choice of the likelihood.

        p.

            #[strong Posterior Predictive Checks]:

        p.

            As a review, if we fit a Bayesian model to data, #[i X], we obtain the posterior distribution \(p(\theta|x)\). We can use this distribution to generate samples from the posterior predictive distribution, which is a probability distribution over possible values of future data x'. In particular, we iterate the following:

        p.

            1. Sample \(\theta_{i}\sim p(\theta|x)\). That is, sample a parameter value from the posterior distribution.

        p.

            2. Sample \(X_{i}'\sim p(x'|\theta_{i})\): that is, sample a data value from the sample distribution conditional on the parameter value from the previous step.

        p.

            #[+url("https://youtu.be/TMnXQ6G6E5Y")]
            
        p.

            #[strong Example]: Suppose that we estimate the prevalence of a certain disease in the UK population using a sample of 10 individuals, where we find that #[i X=1] individual is disease-positive. We use a binomial to explain disease prevalence \(\theta\) in the sample which, along with a uniform prior results in the posterior. We then follow the steps above to generate an approximate posterior predictive distribution, which forecasts the number of disease-positive individuals in a future sample of the same size. In each iteration, #[i i], we collect a paired sample \((\theta_{i},X_{i})\). The set of all samples for the number of disease-cases \(X_{i}\) forms our posterior predictive distribution. This distribution is peaked at \(X=1\) and has an 80% predictive interval of zero to four cases. 

        p.

            By repeating these steps a large number of times we eventually obtain a reasonable approximation to the #[i posterior predictive distribution]. This distribution represents our uncertainty about the outcome of future data and accounts for error in our observed data and our model choice.

        p.

            This two state process reflects the two sources of uncertainty that we have: the uncertainty in the parameter value (reflected in the posterior distribution) and the uncertainty due to sampling variation (reflected by the sampling distribution).

        p.

            If the two above steps are iterated a sufficient number of times, then the resultant histogram of our data samples approaches the shape of the exact posterior predictive distribution. For the majority of cases, we estimate the posterior predictive distribution by sampling since the high dimensional integrals involved are often intractable.

        p.

            #[strong Example]: Lyme disease is a tick-borne infectious disease spread by bacteria of species #[i Borrelia] which are transmitted to ticks when they feed on animal hosts. While fairly common in the eastern US, this disease has been spreading to the western US. Imagine you are researching the occupance of Lyme disease in the western US, and counting the occurrence of the #[Borrelia] bacteria in ticks.


            #[+url("https://media.csuchico.edu/media/Math+450+Lyme+Disease+Example/1_v658mkq6")]

        p.

            You start by assuming the number of ticks with the bacteria in a sample of #[i n] ticks follows a binomial distribution with parameter \(\theta\). Suppose also that from a previous study you think that the value of \(\theta\) is around 10% so you decide that the prior distribution follows a beta with #[i r=5] and #[i s=40]. Give the posterior distribution for \(\theta\).

        p.
            Suppose you sample 40 ticks and find that 7 of them carry the disease. Compute squared-error loss Bayes estimate for the parameter \(\theta\).

        +numericanswer("answer01", "Give the squared-error loss Bayes estimate for the parameter. Round to 2 decimals.", "0.01")

        p.

            Using R, calculate a 90% credible interval for \(\theta\) the probability of a tick having the disease.

        p.

            Using R, obtain an approximate to the #[i posterior predictive distribution]. Give a 80% interval for the number of ticks having this disease in a future sample of #[i n=40].

        +numericanswer("answer02", "Give the lower bound for a future sample value based on the 80% interval. Report a whole integer.", "1")

        +numericanswer("answer03", "Give the upper bound for a future sample value based on the 80% interval. Report a whole integer.", "1")
    
        button(class="btn btn-md btn-primary" type="submit") submit
