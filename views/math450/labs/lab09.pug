extends ../../lab
include ../../mixins/url.pug
include ../../mixins/title.pug
include ../../mixins/userinfo.pug
include ../../mixins/warning.pug
include ../../mixins/shortanswer.pug
include ../../mixins/multiplechoice.pug
include ../../mixins/katex.pug

block head
    +katex()

block content
    +title("MATH 450 Chapter 5-Part 1")
    +warning("Leaving or refreshing this page might lose your work.")

    form(action="/math450/labs/submit" method="post" class="needs-validation" novalidate)
        input(type="hidden" name="_csrf" value= csrfToken)
        input(type="hidden" name="lab" value= labId)
        // Due date is in UTC
        input(type="hidden" name="duedate" value="2020-06-25T07:00:00")
        +userinfo()

        h4 Chapter 5: Priors

        p.

            At the end of this chapter, the reader will know what is meant by a prior and the different philosophies that are used to understand and construct them. Recall Bayes' theorm again: $$p(\theta|data)=\frac{(data|\theta)\times p(\theta)}{p(data)}$$

        p.

            The term \(p(\theta)\) is called the #[i prior distribution]. Priors are the most controversial aspect of Bayesian statistics with opponents criticising their inherent subjectivity.

        p.

            #[strong What are priors and what do they represent?]

        p.

            For Bayesians, the data are treated as fixed and the parameters vary. Bayes' rule tells us that to calculate the posterior probability distribution we must combine a likelihood with a prior probability distribution over parameter values. Gelman et al. suggest that there are two different interpretations of parameter probability distributions: the subjective state of knowledge interpretation where we use a probability to represent our uncertainty over a parameter's true value; and the more objective population interpretation where the parameter's value varies between different samples we take from a population distribution. In both viewpoints, the model parameters are not viewed as static as in frequentist theory.

        p.

            #[strong 5.3.1: Why do we need priors at all?]

        p.

            An obvious question would be if we really need priors. 

        p.

            Bayes' rule is really the only way to update our initial beliefs in light of data.

        p.

            In light of this we must specify an initial belief, otherwise we would have nothing to update.

        p.

            #[strong 5.4 The Explicit Subjectivity of Priors]

        p.

            Opponents of Bayesian approaches to inference criticise the subjectivity involved in choosing a prior. However, all analysis involves a degree of subjectivity, particularly the choice of a statistical model.

        p.

            #[strong Combining a Prior and Likelihood to Form a Posterior]

        p.

            #[strong Example: The fish game]

        p.

            #[+url("https://media.csuchico.edu/media/Math+450+The+Fish+Game/1_6twcgc67")]

        p.

            Suppose that we have a bowl of water with 5 fish where each fish can be either red or white. We want to estimate the total number of red fish in the bowl after we pick out a single fish, and find it to be red. 

        p.

            Before we pulled out the fish we had no strong belief in there being a particular number of red fish and suppose that the numbers 0 through 5 are equally likely and hence have a probability of 1/6 in our discrete prior. 

        p.

            The model for the likelihood of #[i Y] red fish is based on the assumption that all fish are equally likely to be picked. Suppose that the random variable \(X\in\left[0,1\right]\) indicates whether the sampled fish is white or red. What type of likelihood would be suitable for this example?

        p.

            The mechanics of Bayes' rule is shown in the video link below. We start by listing the possible numbers of red fish in the bowl in the leftmost column. The second column specifies our prior probabilities for each of these numbers of red fish. 

        p.

        a: img(src="/math105/fish_table.PNG" alt="Fish Table", style='width:600px;height:350px;')

        p.

            The table above illustrates the straightforward mechanism of Bayes' rule for the case of discrete data. Additionally, when we sum the likelihood over all possible numbers of red fish in the bowl we notice that it equals 3. This demonstrates that a likelihood is not  a valid probability distribution. We also see that the probability that X=0 is 0. Why?

        +shortanswer("answer01", "Why is the probability of X=0 equal to 0?", "2")

        +shortanswer("answer02", "What is the posterior probability for X=3?", "1")

        

        button(class="btn btn-md btn-primary" type="submit") submit
