extends ../../lab
include ../../mixins/url.pug
include ../../mixins/title.pug
include ../../mixins/userinfo.pug
include ../../mixins/warning.pug
include ../../mixins/shortanswer.pug
include ../../mixins/multiplechoice.pug
include ../../mixins/katex.pug

block head
    +katex()

block content
    +title("MATH 450 Chapter 4-Part 2")
    +warning("Leaving or refreshing this page might lose your work.")

    form(action="/math450/labs/submit" method="post" class="needs-validation" novalidate)
        input(type="hidden" name="_csrf" value= csrfToken)
        input(type="hidden" name="lab" value= labId)
        // Due date is in UTC
        input(type="hidden" name="duedate" value="2020-06-25T07:00:00")
        +userinfo()

        h4 Likelihoods Continued

        p. 

            This section introduces us to the idea of likelihood.

        p.

            #[strong Section 4.4: Why Use #[i Likelihood] Rather than Probability?]:

        p.

            #[strong Simple Example]

        p.

            Suppose that we are flipping a coin and recording its outcome. 

        p.

        +multiplechoice("answer01", "What model would we use to represent this?")
            option(selected) choose one
            option Normal 
            option Bernouli/Binomial 
            option Geometric
            option Poisson
            option I think I will just create my own distribution

        p.

            Because of our ignorance about the process of flipping the coin (angle of coin, height of coin, etc) our model cannot perfectly predict the behavior of the coin. The uncertainty means that the model is #[i probabilistic] rather than #[i deterministic]. We might suppose that the coin is fair and thus we would have that \(\theta=0.5\). Furthermore, we assume that the results of each coin flip are #[strong independent]. 

        +shortanswer("answer02", "What does that mean?", "2")  

        +shortanswer("answer03", "What would the probability of obtaining two heads in a row?", "1")  

        p.

            We can calculate the corresponding probabilities of the possible outcomes of the number of heads out of two throws.

        a: img(src="/math105/coin_flip.PNG" alt="Coin toss", style='width:600px;height:250px;')

        p.

            When we assume a particular value of \(\theta\) and vary the data (in this case the number of heads), the collection of resultant probabilities forms a probability distribution. So why do Bayesians insist on calling \(p(data|\theta)\) a #[i likelihood], not a #[ i probability distribution].

        p.

            When we hold the parameters of our model fixed, for example, when we held the probability of a coin landing heads up at \(\theta=\frac{1}{2}\) the resultant distribution of possible samples is a valid distribution because the sum of the probabilities is equal to #[strong one]. 

        p.

            In Bayesian analysis we do not keep the parameters of our model fixed. In Bayesian analysis the data are fixed and the parameters vary. In particular, Bayes' rule tells us how to calculate the posterior probability density for any value of \(\theta\). 

        p.

            Consider flipping a coin whose inherent bias, \(\theta\) is unknown beforehand. In Bayesian inference, we use a sample of coin flips to estimate a posterior belief in any value of \(\theta\). To obtain \(p(data|\theta)\) we must compute \(p(data|\theta)\) in the numerator of Bayes' rule for each possible value of \(\theta\). Suppose that we flipped the coin twice and got 1 heads and 1 tail. What would the probability of this for any value of \(\theta\)? In other words, what would $$P(H,T|\theta)+P(T,H|\theta)=$$

        p.

            This result yields the probability for a fixed data sample (one head and one tail) as a function of \(\theta\). We can graph the value of the above expression for any value of \(\theta\).

        a: img(src="/math105/graph_theta.PNG" alt="Coin toss", style='width:400px;height:250px;')

        p.

            If we took the time to find the area under the curve we would discover that it is not equal to 1 and therefore not a proper probability distribution. #[strong Hence, when we vary \(\theta\), \(p(data|\theta)\) is not a valid probability distribution]. We will then use the term #[i likelihood] to describe \(p(data|\theta)\) when we vary the parameter\(\theta\). 

        p.
        
            #[+url("https://youtu.be/bWaACQGsk7c")]

        p.

            In Bayesian statistics, we always vary the parameter and hold the data fixed. Thus, from a Bayesian perspective, we use the term #[i likelihood] to remind us that \(p(data|\theta)\) is not a probability distribution.

        p. 

            #[strong Example Violent crime counts in New York counties]

        p.

             #[+url("https://media.csuchico.edu/media/Math+450+Violent+Crimes+NY/1_f9rw4gmd")]


        p.

            In the data file likelihood_NewYorkCrimesUnemployment.csv is a data set of the violent crime count and unemployment across New York counties in 2014 (openly available from the New York Criminal Justice website).

        p.

            A simple model here might be to assume that the crime rate count in a particular country is related to the population size by a #[strong Poisson model] $$crime_{i}\sim Poisson(n_{i}\theta)$$ 

        p.

            where \(crime_{i}\) and \(n_{i}\) are the crime count and population in county #[i ]. Derive the likelihood.

        +shortanswer("answer04", "Find the maximum likelihood estimator and then compute it with the actual data. Report your answer to 3 digits", "1") 



        button(class="btn btn-md btn-primary" type="submit") submit
